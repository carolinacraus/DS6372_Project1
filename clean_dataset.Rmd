---
always_allow_html: TRUE
output: 
  html_document: 
    code_folding: hide
  github_document: default
---


<hr>
<center><h2>Explanatory Data Analysis</h2></center>
<hr>

<h4>Loading Data set and inspecting summary statistics as well as missing data:</h4>


```{r, message=FALSE, warning=FALSE, fig.align='center'}
library(tidyverse)
library(naniar)
library(caret)
library(ggcorrplot)
library(FNN)
library(car)
library(ggplot2)
library(rgl)
dataset <- read.csv("C:/Users/Joey/Desktop/applied-stats/datasets/data1.csv")

#glimpse(dataset) # looking at the var types of each

summary(dataset) # visually looking at the sum. stats of the vars in the data. 

vis_miss(dataset) # viz looking @ missing values

```



We can see there is a number of variables that contain NA's

<h4>Var. that contain NA:</h4>

- Engine.HP = 69
- Engine.Cylinders = 30
- Number.of.Doors = 6 

Since many of the missing values seem to be found using any number of automobile spec. websites we will input them into the data set below instead of removing those observations so that we can fully utilize the features and data provided.


<h4>Number of Doors:</h4>
 All NA values here belong to Tesla Model S and Ferrari FF. We were able to find and input these values. 

```{r, message=FALSE, warning=FALSE, fig.align='center'}
#updating NA for Doors
dataset[6931:6935,9] <- 4 # Tesla model S
dataset[4667,9] <- 2 # Ferrari FF 

```



<h4>Engine Cylinders</h4>

The Mazda RX-7, and RX-8 use a unique rotary Engine and thus technically have no cylinders.

On the EV side of the data set we find the remaining NA values and they have been updated to 0 since electric vehicles do not have engine cylinders. future consideration for a substitute value would be ideal here as the automobile market gains more and more Electric vehicles. This substitute value would help to leverage a way to add accurate predictions and values using this metric when comparing many EV.

```{r, message=FALSE, warning=FALSE, fig.align='center'}

dataset[1984:1985, 6] <- 0 # chevy bolt EV
dataset[3717:3720, 6] <- 0 # volkswagen e-golf
dataset[5779:5781,6] <- 0 # mitsubishi i-miev
dataset[8374,6] <- 0 # toyota rav4-ev
dataset[8696:8715, 6] <- 0 # mazda rotary engine RX7 AND RX8



```


<h4>Engine HP</h4>

Certain cars specifically electric vehicles do not report "horsepower" but instead will report Kilowatts (KW)
so we expect to see many Missing values when dealing with Electric vehicles. 

Exceptions to this:
Chevy Spark - GM reports 140 HP
i3 - BMW reports 170-181 HP
i-MiEV - Mitsubishi reports 66 HP
e-golf - Volkswagen reports 115 HP

we could possibly create a ML algo to predict what the HP is... but since they are reported on a different scale, and are of a different type of vehicle
maybe it would be better off changing the "missing values" into 0s. 

```{r, message=FALSE, warning=FALSE, fig.align='center'}
dataset[8374,5] <- 154 # toyota RAV4-EV
dataset[2906:2909,5] <- 305 # lincoln Continental
dataset[4204:4207,5]<- 168 # ford escape
dataset[4915:4920,5]<- 193 # ford freestar
dataset[c(5826,5831,5832,5834,5840,5841),5]<- 305 # chevy impala flex-fuel
dataset[c(6909,6911,6917,6919),5]<-240 # lincoln mkz
dataset[6579, 5] <- 200 # mercedes-benz m-class
dataset[8375:8376,5]<-154 # 2013 rav4 EV and 2014
dataset[540:542, 5] <- 111 # fiat 500 e - from edmunds
dataset[9851:9855,5]<- 109 # kia soul Ev
dataset[4706:4707,5] <- 123 # honda fit Ev
dataset[c(4786,4790,4799),5] <- 143 # ford focus ev
dataset[5779,5] <- 66 # mitsubishi i-miev
dataset[6386:6395,5] <- 107 # nissan leaf 
dataset[6923,5] <- 302 # base tsla model s 2014
dataset[6922,5] <- 302 # 2nd tsla model s 2014
dataset[6925,5] <- 362 # 3rd tsla model s 2014
dataset[6924,5] <- 416 # 4th tsla model s 2014
dataset[6926:6927,5]<- 329 # tsla model s 2015
dataset[6930, 5]<- 362 # tsla model s 2015
dataset[6928,5]<- 416 # tsla model s 2015
dataset[6929,5]<- 691 # tsla model s 2015
dataset[6932,5]<-259 # tsla model s 2016
dataset[6935,5]<-315
dataset[6934:6936,5] <- 373 # tsla model s 2016
dataset[6936,5]<-373# tsla model s 2016
dataset[c(6931,6937),5]<-417# tsla model s 2016
dataset[6939,5]<- 503# tsla model s 2016
dataset[c(6933,6938),5] <- 691# tsla model s 2016

dataset[11322:11324,4] <- 'regular unleaded' # suzuki verona missing fuel type


car_data <- dataset

```


<h4>"Unknown" Data values in the Transmission.Type category</h4>
Values of "Unknown" were discovered in the data. These Unknown values are publicly and easily accessible information and we 
were able to update the database with the proper assignments.


```{r, message=FALSE, warning=FALSE, fig.align='center'}
car_data[1290:1291, 7] <- "AUTOMATIC" # oldsmobile achieva
car_data[4692,7]<- "MANUAL" # cheaper firebird
car_data[4693:4694,7]<- "AUTOMATIC" #more expensive firebird
car_data[6159,7] <- "MANUAL" # GMC Jimmy cheaper 99
car_data[6161,7] <- "AUTOMATIC" # GMC jimmy 99
car_data[6166,7] <- "MANUAL" #gmc jimmy 00
car_data[6175,7]<- "AUTOMATIC" # gmc jimmy 00
car_data[6367,7] <- "MANUAL" # chrysler le baron 
car_data[6369,7] <- "AUTOMATIC"# chrysler le baron 
car_data[c(8043,8044,8047,8048,8049,8050,8052,8054),7] <- "AUTOMATIC" # dodge ram 150

```

There are many chr and int types so we will change them into a factor where needed so that we can adequately plot the data.
```{r, message=FALSE, warning=FALSE, fig.align='center'}

car_data$Make <- as.factor(car_data$Make)
car_data$Engine.Fuel.Type<- as.factor(car_data$Engine.Fuel.Type) # chr
car_data$Engine.HP<- as.numeric(car_data$Engine.HP) # dbl 
car_data$Engine.Cylinders<-as.factor(car_data$Engine.Cylinders) # dbl
car_data$Transmission.Type<-as.factor(car_data$Transmission.Type) # chr
car_data$Driven_Wheels<- as.factor(car_data$Driven_Wheels) # chr
car_data$Number.of.Doors<- as.integer(car_data$Number.of.Doors) # dbl
car_data$Vehicle.Size<-as.factor(car_data$Vehicle.Size) # chr
car_data$Vehicle.Style<-as.factor(car_data$Vehicle.Style) # chr
car_data$Market.Category<-as.factor(car_data$Market.Category) # chr
```




<h4>"Not Applicable" Values:</h4>

upon investigation of the Market Category section we found many "N/A" values (3,742).
These values seem to be implicitly add N/A values, which could in fact mean there is no applicable group that it fits into. we
will change this N/A to "No Category" to avoid any confusion with Missing value data. 

```{r, message=FALSE, warning=FALSE, fig.align='center'}
# change so that we can handle the "N/A" which is NOT an NA
car_data$Market.Category<-as.character(car_data$Market.Category)
sum(car_data$Market.Category == "N/A") #3742 this is our benchmark 

# reading as not applicable we will change to no category
car_data$Market.Category[car_data$Market.Category == "N/A"] <- "No Category"

# sanity check should be 0
sum(car_data$Market.Category == "N/A")

# change to match rest of data type (factor)
car_data$Market.Category<-as.factor(car_data$Market.Category)

```

Typo Audi MPG
```{r, message=FALSE, warning=FALSE, fig.align='center'}

car_data[1120,13] <- 34

```

Finally we will remove the "model" variable since it presents no obvious value.
```{r, message=FALSE, warning=FALSE, fig.align='center'}
#glimpse(car_data) # view the df
car_data <- car_data[,-2] # rm the model column
#glimpse(car_data) # sanity check the df

```

```{r}

car_df <- car_data

# keeping data from > than year 2000
car_df <- car_data %>% filter(Year > 2000)
 
car_df <- car_df %>% filter(MSRP <=90000)

# filter out electric models
car_df <- car_df%>% filter(Engine.Fuel.Type != "electric")

# adjust var types so that they work with our functions below. 
car_df$log_MSRP <- log(car_df$MSRP)
car_df$Year <- as.double(car_df$Year)
car_df$city.mpg<-as.double(car_df$city.mpg)
car_df$highway.MPG<-as.double(car_df$highway.MPG)
car_df$Engine.Cylinders<-as.factor(car_df$Engine.Cylinders)
car_df$Engine.HP<-as.double(car_df$Engine.HP)
levels(car_df$Make)


```


```{r, message=FALSE, warning=FALSE, fig.align='center'}


# removing no-numerical categories 
car_data_correlations <- car_df[,-c(1, 3,5,6,7,9,10,11)]


corr_data <- cor(car_data_correlations)


ggcorrplot(corr_data, outline.color = "black", lab = TRUE)
```




<h3>Test/Validation split on cleaned data</h3>
```{r}
set.seed(7)

trainIndex<-createDataPartition(car_df$log_MSRP,p=.8,list=F)  #p: proportion of data in train

training <- car_df[trainIndex,]
validate <- car_df[-trainIndex,]


```


<h3>Training Data - Simple Model</h3>

- No Feature Selection 
- Used Correlative plot
```{r, message=FALSE, warning=FALSE, fig.align='center'}

car_model_fit <- lm(log(MSRP)~log(Year)+log(Engine.HP), training)

# variance inflation factor
vif(car_model_fit)

# coef. 
summary(car_model_fit)

# diagnostics
plot(car_model_fit)

```


<h3>Simple Model - non-split data set</h3>
```{r, message=FALSE, warning=FALSE, fig.align='center'}


car_model_fit2 <- lm(log(MSRP)~log(Year)+log(Engine.HP),car_df)

summary(car_model_fit2)
plot(car_model_fit2)
plot(car_df$MSRP)

```


Objective 1
- try feat. selection (glm) 
- split data
- interpret coef.

- interpret final model (after CV)
- include hypo tests. (on what)
- confidence intervals of reg. coef. 
- mention practical and stat. sign. of the predictors.
- answer additional questions that are deemed relevant. 


Objective 2
- train/val split OR CV
- create complex model
- compare KNN against complex model created. 
- gather measures of fit for all models (MSE, R^2, Adj.R, AIC, BIC)




<center><h1> ask for clarification </h1></center>

<h4>feat. selection with GLMNET and 5 fold cross validation on entire data set</h4>
```{r}

# 
# set.seed(7)
# 
# fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
# glmnet.fit <- train(log(MSRP)~.,
#                     data = car_df,
#                     method = "glmnet",
#                     trControl = fitControl)
# 
# 
# glmnet.fit
# 
# plot(glmnet.fit)
# opt.pen<- glmnet.fit$finalModel$lambdaOpt
# coef(glmnet.fit$finalModel,opt.pen)
# 
# summary(lm(log(MSRP)~., car_df))

# code to remove the features that glmnet didn't select
# car_glm <- car_df[,!names(car_df) %in% c("MakeAston Martin","MakeBentley", "MakeBugatti", "MakeFerrari", "MakeGMC",
#                                          "MakeLamborghini","MakeMaybach", "MakeMcLaren", "MakeRolls-Royce", 
#                                          "MakeSpyker", "MakeTesla", "Engine.Cylinders6", "Engine.Cylinders16",
#                                          "Market.CategoryCrossover,Exotic,Luxury,High-Performance",
#                                          "Market.CategoryCrossover,Exotic,Luxury,Performance",
#                                          "Market.CategoryExotic,Factory Tuner,High-Performance",
#                                          "Market.CategoryExotic,Factory Tuner,Luxury,High-Performance",
#                                          "Market.CategoryExotic,Factory Tuner,Luxury,Performance",
#                                          "Market.CategoryExotic,Flex Fuel,Factory Tuner,Luxury,High-Performance",
#                                          "Market.CategoryExotic,Flex Fuel,Luxury,High-Performance",
#                                          "Market.CategoryExotic,Luxury", "Market.CategoryExotic,Luxury,High-Performance,Hybrid",
#                                          "Market.CategoryExotic,Luxury,Performance","Market.CategoryFactory Tuner,Luxury",
#                                          "Market.CategoryFlex Fuel,Factory Tuner,Luxury,High-Performance", "Vehicle.StyleCrew Cab Pickup")]
# glimpse(car_df)
# complex_fit <- lm(log(MSRP)~log(as.integer(Year))*log(city.mpg) +log(Engine.HP) ,car_df)
# summary(complex_fit)
# plot(complex_fit)

```


<h4>feat. selection with GLMNET and 5 fold cross validation on TRAINING DATASET</h4>
```{r}
set.seed(7)

fitControl2 <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
glmnet.fit2 <- train(log(MSRP)~.,
                    data = training,
                    method = "glmnet",
                    trControl = fitControl2)


glmnet.fit2
plot(glmnet.fit2)
opt.pen2<- glmnet.fit2$finalModel$lambdaOpt1
coef(glmnet.fit2$finalModel,opt.pen2)

# summary(lm(log(MSRP)~., training))



```


<h4>Complex Model - with training data set</h4>
- no use of feature selection 
- not completely sure how to proceed using the glmnet results or "what" to make more complicated other than intuitive interactions
```{r}
glimpse(training)
complex_fit <- lm(log_MSRP~log(Year)*Market.Category +log(Engine.HP),training)

summary(complex_fit)
plot(complex_fit)

```


<h4>implement KNN regression (or other model)</h4>
```{r}


#removing non-numerical 
training2 <- training[,-c(1,3,5:7,9:11)]

validate2 <- validate[,-c(1,3,5:7,9:11)]

reg1<- knn.reg(train = training2, test = validate2, y=training2$MSRP, k=5)


```



<h4> 5 fold cv and training model using knn </h4>
```{r}
# 5 fold CV 
fitControl2<-trainControl(method="repeatedcv", number = 5, repeats=1)

# training model with 5 fold CV
knn.fit<-train(log(MSRP)~log(Year)+log(Engine.HP),
               data = training,
               method = "knn",
               trControl = fitControl2)

plot(knn.fit)
knn.fit # table of k and stats

preds<- predict(knn.fit, validate2)
pred.surface<- matrix(preds)
plot3d(training$Year,training$MSRP,training$Engine.HP)
#surface3d(pred.surface, alpha=.4) # how can we use surface with variables? or do we NEED to define columns/rows etc?

```



